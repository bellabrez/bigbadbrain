{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each good fly, I want to run a few basic analyses:\n",
    "    - Plot PC 2D Hist maps (first ~50-100 PCs)\n",
    "    - Foward and Rotational R2 predictions for a distribution of num_PCs\n",
    "    - Some visualization of which 2D maps are being used by each model\n",
    "    - Spatial PC distribution for forward and rotation\n",
    "    \n",
    "#### Let's order the steps.\n",
    "    Q1: What is best num_pcs to use across all flies?\n",
    "        - Load a fly's PCs\n",
    "        - Predict with different #PCs\n",
    "        - Save coef figure\n",
    "        - Append R2s\n",
    "        - Finally, compare R2 distribution across flies\n",
    "    Q2: For that num_pcs, compare the 2Dhist plots across flies with high/different weights from forward and turning.\n",
    "        - \n",
    "        \n",
    "#### Final figures:\n",
    "    - Show predicted behavior on completely held out data\n",
    "        - ie, predict with standard CV or whatever, but only give the model 90% of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages/lib/python/')\n",
    "import ants\n",
    "import bigbadbrain as bbb\n",
    "from scipy.linalg import toeplitz\n",
    "import scipy.linalg as sl\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.signal import convolve\n",
    "import sklearn\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from skimage.filters import threshold_triangle\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages')\n",
    "import cv2\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interp_fictrac(fictrac, behavior, fps, resolution, expt_len, timestamps, interp_to):\n",
    "    camera_rate = 1/fps * 1000 # camera frame rate in ms\n",
    "    sphere_radius = 4.5e-3\n",
    "    filter_window = 51\n",
    "    \n",
    "    x_original = np.arange(0,expt_len,camera_rate)\n",
    "    \n",
    "    if behavior == 'all':\n",
    "        dx = np.asarray(fictrac['dRotLabX'])\n",
    "        dy = np.asarray(fictrac['dRotLabY'])\n",
    "        dz = np.asarray(fictrac['dRotLabZ'])\n",
    "        dx = scipy.signal.savgol_filter(dx,filter_window,3) * sphere_radius * 50 * 100 * 10\n",
    "        dy = scipy.signal.savgol_filter(dy,filter_window,3) * sphere_radius * 50 * 100 * 10\n",
    "        dz = scipy.signal.savgol_filter(dz,filter_window,3) * 180 / np.pi * 50\n",
    "        fictrac_smoothed = np.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "    elif behavior == 'Y':\n",
    "        dy = np.asarray(fictrac['dRotLabY'])\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(dy,filter_window,3) * sphere_radius * 50 * 100 * 10\n",
    "    elif behavior == 'Z':\n",
    "        dz = np.asarray(fictrac['dRotLabZ'])\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(dz,filter_window,3) * 180 / np.pi * 50\n",
    "    else:\n",
    "        print('invalid behavior')\n",
    "    \n",
    "    #fictrac_smoothed = np.abs(fictrac_smoothed)\n",
    "    fictrac_interp_temp = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "    xnew = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "    \n",
    "    if interp_to is 'timestamps':\n",
    "        fictrac_interp = fictrac_interp_temp(timestamps[:,25])\n",
    "    elif interp_to is 'xnew':\n",
    "        fictrac_interp = fictrac_interp_temp(xnew)\n",
    "    else:\n",
    "        print('Invalid interp_to ({})'.format(interp_to))\n",
    "\n",
    "    # Replace Nans with zeros (for later code)\n",
    "    np.nan_to_num(fictrac_interp, copy=False);\n",
    "    \n",
    "    return fictrac_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_r2(true, prediction):\n",
    "    u = np.sum((prediction-true)**2)\n",
    "    v = np.sum((true-np.mean(true))**2)\n",
    "    return 1 - u / v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_directory = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flies = ['fly_1',\n",
    "         'fly_3',\n",
    "         'fly_5',\n",
    "         'fly_7',\n",
    "         'fly_19',\n",
    "         'fly_21',\n",
    "         'fly_48',\n",
    "         'fly_51',\n",
    "         'fly_54',\n",
    "         'fly_68']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 1.36 sec\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.36 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 3s\n",
      "Num PCs: 50 | Dur: 4s\n",
      "Num PCs: 75 | Dur: 7s\n",
      "Num PCs: 100 | Dur: 9s\n",
      "Num PCs: 200 | Dur: 19s\n",
      "Num PCs: 300 | Dur: 33s\n",
      "Num PCs: 500 | Dur: 66s\n",
      "Num PCs: 1000 | Dur: 180s\n",
      "Num PCs: 2000 | Dur: 737s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 359.72 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.08 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 1s\n",
      "Num PCs: 50 | Dur: 2s\n",
      "Num PCs: 75 | Dur: 3s\n",
      "Num PCs: 100 | Dur: 4s\n",
      "Num PCs: 200 | Dur: 8s\n",
      "Num PCs: 300 | Dur: 13s\n",
      "Num PCs: 500 | Dur: 23s\n",
      "Num PCs: 1000 | Dur: 65s\n",
      "Num PCs: 2000 | Dur: 376s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 1.18 sec\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.05 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 2s\n",
      "Num PCs: 50 | Dur: 3s\n",
      "Num PCs: 75 | Dur: 6s\n",
      "Num PCs: 100 | Dur: 8s\n",
      "Num PCs: 200 | Dur: 17s\n",
      "Num PCs: 300 | Dur: 27s\n",
      "Num PCs: 500 | Dur: 49s\n",
      "Num PCs: 1000 | Dur: 115s\n",
      "Num PCs: 2000 | Dur: 359s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 13.49 min\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.23 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 4s\n",
      "Num PCs: 50 | Dur: 9s\n",
      "Num PCs: 75 | Dur: 12s\n",
      "Num PCs: 100 | Dur: 24s\n",
      "Num PCs: 200 | Dur: 61s\n",
      "Num PCs: 300 | Dur: 76s\n",
      "Num PCs: 500 | Dur: 137s\n",
      "Num PCs: 1000 | Dur: 346s\n",
      "Num PCs: 2000 | Dur: 1561s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 67.15 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.12 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 1s\n",
      "Num PCs: 50 | Dur: 2s\n",
      "Num PCs: 75 | Dur: 4s\n",
      "Num PCs: 100 | Dur: 6s\n",
      "Num PCs: 200 | Dur: 13s\n",
      "Num PCs: 300 | Dur: 21s\n",
      "Num PCs: 500 | Dur: 40s\n",
      "Num PCs: 1000 | Dur: 107s\n",
      "Num PCs: 2000 | Dur: 441s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 77.66 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.01 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 2s\n",
      "Num PCs: 50 | Dur: 5s\n",
      "Num PCs: 75 | Dur: 8s\n",
      "Num PCs: 100 | Dur: 13s\n",
      "Num PCs: 200 | Dur: 27s\n",
      "Num PCs: 300 | Dur: 42s\n",
      "Num PCs: 500 | Dur: 81s\n",
      "Num PCs: 1000 | Dur: 236s\n",
      "Num PCs: 2000 | Dur: 855s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 88.48 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.13 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 1s\n",
      "Num PCs: 50 | Dur: 3s\n",
      "Num PCs: 75 | Dur: 4s\n",
      "Num PCs: 100 | Dur: 6s\n",
      "Num PCs: 200 | Dur: 15s\n",
      "Num PCs: 300 | Dur: 23s\n",
      "Num PCs: 500 | Dur: 41s\n",
      "Num PCs: 1000 | Dur: 110s\n",
      "Num PCs: 2000 | Dur: 536s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 49.44 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.07 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 1s\n",
      "Num PCs: 50 | Dur: 2s\n",
      "Num PCs: 75 | Dur: 4s\n",
      "Num PCs: 100 | Dur: 5s\n",
      "Num PCs: 200 | Dur: 13s\n",
      "Num PCs: 300 | Dur: 24s\n",
      "Num PCs: 500 | Dur: 51s\n",
      "Num PCs: 1000 | Dur: 154s\n",
      "Num PCs: 2000 | Dur: 583s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 138.88 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.46 sec\n",
      "Num PCs: 10 | Dur: 0s\n",
      "Num PCs: 25 | Dur: 2s\n",
      "Num PCs: 50 | Dur: 6s\n",
      "Num PCs: 75 | Dur: 10s\n",
      "Num PCs: 100 | Dur: 14s\n",
      "Num PCs: 200 | Dur: 29s\n",
      "Num PCs: 300 | Dur: 46s\n",
      "Num PCs: 500 | Dur: 90s\n",
      "Num PCs: 1000 | Dur: 215s\n",
      "Num PCs: 2000 | Dur: 608s\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Failed. Extracting frame timestamps from bruker xml file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 18.26 sec\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.30 sec\n",
      "Num PCs: 10 | Dur: 1s\n",
      "Num PCs: 25 | Dur: 2s\n",
      "Num PCs: 50 | Dur: 4s\n",
      "Num PCs: 75 | Dur: 6s\n",
      "Num PCs: 100 | Dur: 9s\n",
      "Num PCs: 200 | Dur: 18s\n",
      "Num PCs: 300 | Dur: 28s\n",
      "Num PCs: 500 | Dur: 56s\n",
      "Num PCs: 1000 | Dur: 154s\n",
      "Num PCs: 2000 | Dur: 528s\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for fly in flies:\n",
    "    directory = os.path.join(root_directory, fly, 'func_0')\n",
    "\n",
    "    timestamps = bbb.load_timestamps(os.path.join(directory, 'imaging'))\n",
    "    fictrac = bbb.load_fictrac(os.path.join(directory, 'fictrac'))\n",
    "    pca_loadings = np.load(os.path.join(directory, 'pca', 'loadings_(temporal).npy'))\n",
    "\n",
    "    ### Interp fictrac ###\n",
    "\n",
    "    resolution = 50 #desired resolution in ms\n",
    "    expt_len = 1000*30*60\n",
    "    fps = 50 #of fictrac camera\n",
    "    xnew = np.arange(0,expt_len,resolution)\n",
    "\n",
    "    fictracs = {}\n",
    "    for behavior in ['all', 'Y', 'Z']:\n",
    "        fictracs[behavior] = interp_fictrac(fictrac, behavior, fps, resolution, expt_len, timestamps, 'timestamps')\n",
    "\n",
    "    pca_loadings_std = np.std(pca_loadings,axis=0)\n",
    "    pca_loadings= np.divide(pca_loadings,pca_loadings_std)\n",
    "\n",
    "    for behavior in ['Y', 'Z']:\n",
    "        fictracs_std = np.std(fictracs[behavior])\n",
    "        fictracs[behavior] = np.divide(fictracs[behavior],fictracs_std)\n",
    "\n",
    "    Y_glm = np.vstack((fictracs['Y'], fictracs['Z'])).T\n",
    "\n",
    "    for num_pcs in [10,25,50,75,100,200,300,500,1000,2000]:\n",
    "        t0 = time()\n",
    "        models.append({'num_pcs': num_pcs,\n",
    "                       'model': MultiTaskLassoCV(),\n",
    "                       'fly': fly})\n",
    "        X_glm = pca_loadings[:,:num_pcs]\n",
    "\n",
    "        # Fit model\n",
    "        models[-1]['model'].fit(X_glm, Y_glm)\n",
    "\n",
    "        # Get scores\n",
    "        prediction = models[-1]['model'].predict(X_glm)\n",
    "        models[-1]['score_Y'] = get_r2(fictracs['Y'], prediction[:,0])\n",
    "        models[-1]['score_Z'] = get_r2(fictracs['Z'], prediction[:,1])\n",
    "        models[-1]['score'] = models[-1]['model'].score(X_glm, Y_glm)\n",
    "\n",
    "        print('Num PCs: {} | Dur: {:0.0f}s'.format(num_pcs, time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_preprocess_data',\n",
       " '_set_intercept',\n",
       " 'alpha_',\n",
       " 'alphas',\n",
       " 'alphas_',\n",
       " 'coef_',\n",
       " 'copy_X',\n",
       " 'cv',\n",
       " 'dual_gap_',\n",
       " 'eps',\n",
       " 'fit',\n",
       " 'fit_intercept',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'max_iter',\n",
       " 'mse_path_',\n",
       " 'n_alphas',\n",
       " 'n_iter_',\n",
       " 'n_jobs',\n",
       " 'normalize',\n",
       " 'path',\n",
       " 'positive',\n",
       " 'precompute',\n",
       " 'predict',\n",
       " 'random_state',\n",
       " 'score',\n",
       " 'selection',\n",
       " 'set_params',\n",
       " 'tol',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models[0]['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coef_Y': [0.006240564478381373,\n",
       "  -0.03696323110700434,\n",
       "  0.14803076809136456,\n",
       "  -0.15173859802020975,\n",
       "  -0.14873608026794394,\n",
       "  0.13252173140125548,\n",
       "  -0.29176597557177414,\n",
       "  0.025970095376289914,\n",
       "  -0.036007684667967235,\n",
       "  0.304120778465223],\n",
       " 'coef_Z': [-0.007909465988064725,\n",
       "  -0.01481168304495498,\n",
       "  0.0023804978116918236,\n",
       "  -0.08072709164152847,\n",
       "  -0.018146639385701536,\n",
       "  -0.02159561494579837,\n",
       "  -0.027564312286649854,\n",
       "  -0.026412477101620854,\n",
       "  -0.002115063041530677,\n",
       "  0.08179584845213565],\n",
       " 'fly': 'fly_1',\n",
       " 'num_pcs': 10,\n",
       " 'score': 0.15713249817922123,\n",
       " 'score_Y': 0.29609411843721123,\n",
       " 'score_Z': 0.018170877921231066}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_for_saving[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_for_saving = []\n",
    "for model in models:\n",
    "    new_model = {}\n",
    "    new_model['fly'] = model['fly']\n",
    "    new_model['num_pcs'] = model['num_pcs']\n",
    "    new_model['score'] = model['score']\n",
    "    new_model['score_Y'] = model['score_Y']\n",
    "    new_model['score_Z'] = model['score_Z']\n",
    "    new_model['coef_Y'] = model['model'].coef_[0,:].tolist()\n",
    "    new_model['coef_Z'] = model['model'].coef_[1,:].tolist()\n",
    "    models_for_saving.append(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = os.path.join(root_directory, '20191106_analysis', '20191106_models.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_file, 'w') as outfile:\n",
    "    json.dump(models_for_saving, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
