{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages/lib/python/')\n",
    "import ants\n",
    "import os\n",
    "import bigbadbrain as bbb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from skimage.filters import threshold_triangle\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages')\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import cv2\n",
    "import matplotlib.patches as mpatches\n",
    "import psutil\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import itertools\n",
    "import random\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "from scipy.fftpack import fft,fftshift,ifft\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 59.30 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 67.10 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 47.89 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 69.38 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 52.91 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 73.11 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 39.07 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 56.76 ms\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 169.16 ms\n"
     ]
    }
   ],
   "source": [
    "fly_names = ['fly_087', 'fly_089', 'fly_094', 'fly_097', 'fly_098', 'fly_099', 'fly_100', 'fly_101', 'fly_105']\n",
    "dataset_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "expt_len = 1000*30*60\n",
    "resolution = 10\n",
    "high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "\n",
    "class Fly:\n",
    "    def __init__ (self, fly_name, fly_idx):\n",
    "        self.dir = os.path.join(dataset_path, fly_name, 'func_0')\n",
    "        self.fly_idx = fly_idx\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "    def load_timestamps (self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "    def load_fictrac (self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "    def load_brain_slice (self):\n",
    "        self.brain = brain[:,:,:,self.fly_idx]\n",
    "    def load_anatomy (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', 'anat-to-meanbrain.nii')\n",
    "        self.anatomy = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def load_z_depth_correction (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', '20201220_warped_z_depth.nii')\n",
    "        self.z_correction = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def get_cluster_averages (self, cluster_model_labels, n_clusters):\n",
    "        neural_data = self.brain.reshape(-1, 3384)\n",
    "        signals = []\n",
    "        self.cluster_indicies = []\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model_labels==cluster_num)[0]\n",
    "            mean_signal = np.mean(neural_data[cluster_indicies,:], axis=0)\n",
    "            signals.append(mean_signal)\n",
    "            self.cluster_indicies.append(cluster_indicies) # store for later\n",
    "        self.cluster_signals=np.asarray(signals)\n",
    "    def get_cluster_id (self, x, y):\n",
    "        ax_vec = x*128 + y\n",
    "        for i in range(n_clusters):\n",
    "            if ax_vec in self.cluster_indicies[i]:\n",
    "                cluster_id = i\n",
    "                break\n",
    "        return cluster_id\n",
    "\n",
    "#####################\n",
    "### Load Master X ###\n",
    "#####################\n",
    "file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20210316_neural_weighted_behavior/master_X.npy\"\n",
    "X = np.load(file)\n",
    "\n",
    "#####################\n",
    "### Make Clusters ###\n",
    "#####################\n",
    "n_clusters = 2000\n",
    "labels_file = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/cluster_labels.npy'\n",
    "cluster_model_labels_all = np.load(labels_file)\n",
    "\n",
    "###################\n",
    "### Build Flies ###\n",
    "###################\n",
    "flies = {}\n",
    "for i, fly in enumerate(fly_names):\n",
    "    flies[fly] = Fly(fly_name=fly, fly_idx=i)\n",
    "    flies[fly].load_timestamps()\n",
    "    flies[fly].load_z_depth_correction()\n",
    "\n",
    "z = 29\n",
    "\n",
    "#######################\n",
    "### Load Superslice ###\n",
    "#######################\n",
    "brain_file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/superslice_{}.nii\".format(z) #<---------- !!!\n",
    "brain = np.array(nib.load(brain_file).get_data(), copy=True)\n",
    "fly_idx_delete = 3 #(fly_095)\n",
    "brain = np.delete(brain, fly_idx_delete, axis=-1) #### DELETING FLY_095 ####\n",
    "\n",
    "# Get cluster responses for this slice\n",
    "for fly in fly_names:\n",
    "    flies[fly].load_brain_slice()\n",
    "    flies[fly].get_cluster_averages(cluster_model_labels_all[z,:], n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "### Main Loop ###\n",
    "#################\n",
    "X_all_clusters = []\n",
    "Y_all_clusters = []\n",
    "for cluster_num in range(50):#range(n_clusters):\n",
    "    #if cluster_num%100 == 0:\n",
    "    print(str(cluster_num))\n",
    "        \n",
    "    ###############################################################\n",
    "    ### Build Y vector for a single supervoxel (with all flies) ###\n",
    "    ###############################################################\n",
    "    all_fly_neural = []\n",
    "    for fly in fly_names:\n",
    "        signal = flies[fly].cluster_signals[cluster_num,:]\n",
    "        all_fly_neural.extend(signal)\n",
    "    Y = np.asarray(all_fly_neural)\n",
    "    Y_all_clusters.append(Y)\n",
    "\n",
    "    ###########################################\n",
    "    ### Build the X matrix for this cluster ###\n",
    "    ###########################################\n",
    "    # For each fly, this cluster could have originally come from a different z-depth\n",
    "    # Get correct original z-depth\n",
    "    Xs_new = []\n",
    "    for i, fly in enumerate(fly_names):\n",
    "        cluster_indicies = flies[fly].cluster_indicies[cluster_num]\n",
    "        z_map = flies[fly].z_correction[:,:,z].ravel()\n",
    "        original_z = int(np.median(z_map[cluster_indicies]))\n",
    "        Xs_new.append(X[original_z,i,:,:])\n",
    "    Xs_new = np.asarray(Xs_new)\n",
    "    X_cluster = np.reshape(np.moveaxis(Xs_new,0,1),(-1,30456))\n",
    "\n",
    "    X_all_clusters.append(X_cluster)\n",
    "    \n",
    "#     ###################\n",
    "#     ### Dot Product ###\n",
    "#     ###################\n",
    "#     cluster_response = np.dot(X_cluster,Y)\n",
    "#     cluster_responses.append(cluster_response)\n",
    "# cluster_responses = np.asarray(cluster_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all_clusters = np.asarray(Y_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 30456)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_clusters = np.asarray(X_all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2000, 30456)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all_clusters = X_all_clusters[:,:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#20ms is 50 per sec with 10 sec is 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 30456), (50, 500, 30456))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all_clusters.shape, X_all_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_clusters = np.reshape(X_all_clusters, (25000, 30456))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all_clusters = np.reshape(Y_all_clusters, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
