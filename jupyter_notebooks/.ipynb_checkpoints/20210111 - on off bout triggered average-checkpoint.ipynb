{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages/lib/python/')\n",
    "import ants\n",
    "import os\n",
    "import bigbadbrain as bbb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from skimage.filters import threshold_triangle\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages')\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import cv2\n",
    "import matplotlib.patches as mpatches\n",
    "import psutil\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import itertools\n",
    "import random\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fly_names = ['fly_087', 'fly_089', 'fly_094', 'fly_097', 'fly_098', 'fly_099', 'fly_100', 'fly_101', 'fly_105']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expt_len = 1000*30*60\n",
    "resolution = 10\n",
    "high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "z=20\n",
    "behaviors = ['Y_pos', 'Y_neg', 'Z_pos', 'Z_neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Neural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 13.813099145889282\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "brain_file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201110_analysis_superfly_supervoxels/superslice_20.nii\"\n",
    "brain = np.array(nib.load(brain_file).get_data(), copy=True)\n",
    "print(f'Duration: {time.time()-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anat_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/anat_templates/luke.nii\"\n",
    "anatomy = np.array(nib.load(anat_path).get_data(), copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_clusters(brain, n_clusters):\n",
    "    t0 = time.time()\n",
    "    clustering_dir = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201110_analysis_superfly_supervoxels\"\n",
    "    super_to_cluster = brain.reshape(-1, 33840)\n",
    "    connectivity = grid_to_graph(256,128)\n",
    "    cluster_model = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "                                    memory=clustering_dir,\n",
    "                                    linkage='ward',\n",
    "                                    connectivity=connectivity)\n",
    "    cluster_model.fit(super_to_cluster)\n",
    "    print('Duration: {}'.format(time.time()-t0))\n",
    "    return cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 54.75799345970154\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 2000\n",
    "cluster_model = create_clusters(brain, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Flies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fly:\n",
    "    def __init__ (self, fly_name, fly_idx):\n",
    "        self.dir = os.path.join(dataset_path, fly_name, 'func_0')\n",
    "        self.fly_idx = fly_idx\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "    def load_timestamps (self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "    def load_fictrac (self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "    def load_brain_slice (self):\n",
    "        self.brain = brain[:,:,:,self.fly_idx]\n",
    "    def load_anatomy (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', 'anat-to-meanbrain.nii')\n",
    "        self.anatomy = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def get_cluster_averages (self, cluster_model, n_clusters):\n",
    "        neural_data = self.brain.reshape(-1, 3384)\n",
    "        signals = []\n",
    "        self.cluster_indicies = []\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model.labels_==cluster_num)[0]\n",
    "            mean_signal = np.mean(neural_data[cluster_indicies,:], axis=0)\n",
    "            signals.append(mean_signal)\n",
    "            self.cluster_indicies.append(cluster_indicies) # store for later\n",
    "        self.cluster_signals=np.asarray(signals)\n",
    "    def make_corr_map (self, n_clusters, cluster_model, behavior):\n",
    "        corrs = []\n",
    "        # remove zeros from correlation\n",
    "        behavior_vector = flies[fly].fictrac.fictrac[behavior]\n",
    "        non_zero_entries = np.where(behavior_vector != 0)[0]\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model.labels_==i)[0]\n",
    "            if len(cluster_indicies) > 2000:\n",
    "                corrs.append(0)\n",
    "            else:\n",
    "                corrs.append(scipy.stats.pearsonr(behavior_vector[non_zero_entries],\n",
    "                                                  self.cluster_signals[i,non_zero_entries])[0])\n",
    "        colored_by_betas = np.zeros(256*128)\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model.labels_==cluster_num)[0]\n",
    "            colored_by_betas[cluster_indicies] = corrs[cluster_num]\n",
    "        colored_by_betas = colored_by_betas.reshape(256,128)\n",
    "        self.maps[behavior] = colored_by_betas\n",
    "    def get_cluster_id (self, x, y):\n",
    "        ax_vec = x*128 + y\n",
    "        for i in range(n_clusters):\n",
    "            if ax_vec in self.cluster_indicies[i]:\n",
    "                cluster_id = i\n",
    "                break\n",
    "        return cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fictrac:\n",
    "    def __init__ (self, fly_dir, timestamps):\n",
    "        self.fictrac_raw = bbb.load_fictrac(os.path.join(fly_dir, 'fictrac'))\n",
    "        self.timestamps = timestamps\n",
    "    def make_interp_object(self, behavior):\n",
    "        # Create camera timepoints\n",
    "        fps=50\n",
    "        camera_rate = 1/fps * 1000 # camera frame rate in ms\n",
    "        expt_len = 1000*30*60\n",
    "        x_original = np.arange(0,expt_len,camera_rate)\n",
    "\n",
    "        # Smooth raw fictrac data\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(np.asarray(self.fictrac_raw[behavior]),25,3)\n",
    "\n",
    "        # Create interp object with camera timepoints\n",
    "        fictrac_interp_object = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "        return fictrac_interp_object\n",
    "\n",
    "    def pull_from_interp_object(self, interp_object, timepoints):\n",
    "        new_interp = interp_object(timepoints)\n",
    "        np.nan_to_num(new_interp, copy=False);\n",
    "        return new_interp\n",
    "\n",
    "    def interp_fictrac(self, z):\n",
    "        behaviors = ['dRotLabY', 'dRotLabZ']; shorts = ['Y', 'Z']\n",
    "        self.fictrac = {}\n",
    "\n",
    "        for behavior, short in zip(behaviors, shorts):\n",
    "            interp_object = self.make_interp_object(behavior)\n",
    "            self.fictrac[short + 'i'] = interp_object\n",
    "\n",
    "            ### Velocity ###\n",
    "            low_res_behavior = self.pull_from_interp_object(interp_object, self.timestamps[:,z])\n",
    "            self.fictrac[short] = low_res_behavior#/np.std(low_res_behavior)\n",
    "            \n",
    "            ### Clipped Velocities ###\n",
    "            self.fictrac[short + '_pos'] = np.clip(self.fictrac[short], a_min=0, a_max=None)\n",
    "            self.fictrac[short + '_neg'] = np.clip(self.fictrac[short], a_min=None, a_max=0)*-1\n",
    "            \n",
    "            ### Strongly Clipped Velocities ###\n",
    "            # excludes points even close to 0\n",
    "            #self.fictrac[short + '_pos_very'] = np.clip(self.fictrac[short], a_min=0.3, a_max=None)\n",
    "            #self.fictrac[short + '_neg_very'] = np.clip(self.fictrac[short], a_min=None, a_max=-0.3)*-1\n",
    "\n",
    "            ### Acceleration ###\n",
    "            high_res_behavior = self.pull_from_interp_object(interp_object, high_res_timepoints)\n",
    "            self.fictrac[short + 'h'] = high_res_behavior/np.std(high_res_behavior)\n",
    "\n",
    "            accel = scipy.signal.savgol_filter(np.diff(high_res_behavior),25,3)\n",
    "            accel = np.append(accel, 0)\n",
    "            interp_object = interp1d(high_res_timepoints, accel, bounds_error = False)\n",
    "            acl = interp_object(self.timestamps[:,z])\n",
    "            acl[-1] = 0\n",
    "            self.fictrac[short + 'a'] = acl#/np.std(acl)\n",
    "            \n",
    "            ### Clipped Acceleration ###\n",
    "            self.fictrac[short + 'a' + '_pos'] = np.clip(self.fictrac[short + 'a'], a_min=0, a_max=None)\n",
    "            self.fictrac[short + 'a' + '_neg'] = np.clip(self.fictrac[short + 'a'], a_min=None, a_max=0)*-1\n",
    "\n",
    "        self.fictrac['YZ'] = np.sqrt(np.power(self.fictrac['Y'],2), np.power(self.fictrac['Z'],2))\n",
    "        self.fictrac['YZh'] = np.sqrt(np.power(self.fictrac['Yh'],2), np.power(self.fictrac['Zh'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===fly_087===\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 12.66 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.37 sec\n",
      "===fly_089===\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 12.61 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.46 sec\n",
      "===fly_094===\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 275.12 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.63 sec\n",
      "===fly_095===\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 7.47 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.46 sec\n"
     ]
    }
   ],
   "source": [
    "flies = {}\n",
    "for i, fly in enumerate(fly_names):\n",
    "    print('==={}==='.format(fly))\n",
    "    flies[fly] = Fly(fly_name=fly, fly_idx=i)\n",
    "    flies[fly].load_timestamps()\n",
    "    flies[fly].load_fictrac()\n",
    "    flies[fly].fictrac.interp_fictrac(z)\n",
    "    flies[fly].load_brain_slice()\n",
    "    flies[fly].load_anatomy()\n",
    "    flies[fly].get_cluster_averages(cluster_model, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Pool behavior\n",
    "not_clipped_behaviors = ['Y', 'Z', 'Ya', 'Za']\n",
    "clipped_behaviors = ['Y_pos', 'Y_neg',\n",
    "                     'Z_pos', 'Z_neg',\n",
    "                     'Ya_pos', 'Ya_neg',\n",
    "                     'Za_pos', 'Za_neg']\n",
    "all_behaviors = not_clipped_behaviors + clipped_behaviors\n",
    "\n",
    "pooled_behavior = {}\n",
    "for j, behavior in enumerate(all_behaviors):\n",
    "    pooled_behavior[behavior] = []\n",
    "    for i,fly in enumerate(flies):\n",
    "        pooled_behavior[behavior].append(flies[fly].fictrac.fictrac[behavior])\n",
    "    pooled_behavior[behavior] = np.asarray(pooled_behavior[behavior]).flatten()\n",
    "\n",
    "### Correct behavior stddev as a pooled group\n",
    "stds = {}\n",
    "for j, behavior in enumerate(not_clipped_behaviors):\n",
    "    stds[behavior] = np.std(pooled_behavior[behavior])\n",
    "\n",
    "for j, behavior in enumerate(all_behaviors):\n",
    "    std_key = behavior.split('_')[0] # grab not split key\n",
    "    for i,fly in enumerate(flies):\n",
    "        flies[fly].fictrac.fictrac[behavior] = flies[fly].fictrac.fictrac[behavior]/stds[std_key]\n",
    "    pooled_behavior[behavior] = pooled_behavior[behavior]/stds[std_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bouts(fly):\n",
    "    expt_len = 1000*30*60\n",
    "    resolution = 10\n",
    "    high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "\n",
    "    behavior = 'Yh'\n",
    "\n",
    "    B_THRESHOLD = np.std(flies[fly].fictrac.fictrac[behavior])/4\n",
    "    ALIVE_TIME = 1000 # in ms\n",
    "    DEAD_TIME = 1000 # in ms\n",
    "\n",
    "    state = 'quiescent'\n",
    "    up_streak = 0\n",
    "    down_streak = 0\n",
    "    BOUTS = []\n",
    "    ALIVE_TIME = int(ALIVE_TIME/resolution)\n",
    "    DEAD_TIME = int(DEAD_TIME/resolution)\n",
    "\n",
    "    for i in range(len(flies[fly].fictrac.fictrac[behavior])):\n",
    "        # If high behavior, reset down_streak, and add 1 to up_streak\n",
    "        if flies[fly].fictrac.fictrac[behavior][i] > B_THRESHOLD:\n",
    "            down_streak = 0\n",
    "            up_streak += 1\n",
    "        else:\n",
    "            up_streak = 0\n",
    "            down_streak += 1\n",
    "\n",
    "        if state == 'quiescent':\n",
    "            if up_streak >= ALIVE_TIME:\n",
    "                state = 'moving'\n",
    "                BOUTS.append({'start': i-ALIVE_TIME})\n",
    "        elif state == 'moving':\n",
    "            if down_streak >= DEAD_TIME:\n",
    "                state = 'quiescent'\n",
    "                BOUTS[-1]['end'] = i-DEAD_TIME\n",
    "    BOUTS = [bout for bout in BOUTS if 'end' in bout]\n",
    "    #print('Found {} bouts'.format(len(BOUTS)))\n",
    "\n",
    "    ### Clean Start Bouts\n",
    "    # remove bouts that have behavior too close *before* them\n",
    "    before = 1000 # in ms\n",
    "    before = int(before/10)\n",
    "    start_bouts=[bout for bout in BOUTS if np.mean(np.abs(flies[fly].fictrac.fictrac[behavior][bout['start']-before:bout['start']])) < .2]\n",
    "    #print('Remaining start_bouts post-cleaning: {}'.format(np.shape(start_bouts)[0]))\n",
    "\n",
    "    ### Clean Stop Bouts\n",
    "    # remove bouts that have behavior too close *after* them\n",
    "    before = 1000 # in ms\n",
    "    before = int(before/10)\n",
    "    stop_bouts=[bout for bout in BOUTS if np.mean(np.abs(flies[fly].fictrac.fictrac[behavior][bout['end']:bout['end']+before])) < .2]\n",
    "    #print('Remaining stop_bouts bouts post-cleaning: {}'.format(np.shape(stop_bouts)[0]))\n",
    "    return start_bouts, stop_bouts\n",
    "\n",
    "def bout_triggered(fly, neural_data, all_bouts, bout_type):\n",
    "    if bout_type == 'start_bouts':\n",
    "        align_to = 'start'\n",
    "    elif bout_type == 'stop_bouts':\n",
    "        align_to = 'end'\n",
    "    before = 3000 #in ms\n",
    "    after = 3000 # in ms\n",
    "    jump = flies[fly].timestamps[1,0]-flies[fly].timestamps[0,0]\n",
    "    num_neural_points = int(before/jump)\n",
    "\n",
    "    before = int(before/10) # now everything is in units of 10ms\n",
    "    after = int(after/10)\n",
    "    bins = bbb.create_bins(10,before*10,after*10)[:-1]\n",
    "\n",
    "    xss = []; yss = []\n",
    "    for i in range(len(all_bouts[bout_type])):\n",
    "        nearest = np.searchsorted(flies[fly].timestamps[:,z]/10, all_bouts[bout_type][i][align_to])\n",
    "        offset = (flies[fly].timestamps[nearest,z]/10 - all_bouts[bout_type][i][align_to])*10\n",
    "        xs = np.arange(offset-num_neural_points*jump,offset+num_neural_points*jump,jump)\n",
    "        ys = neural_data[nearest-num_neural_points:nearest+num_neural_points]\n",
    "        if len(ys) == 10:\n",
    "            xss.append(xs); yss.append(ys)\n",
    "    xss = np.asarray(xss); yss = np.asarray(yss)\n",
    "    \n",
    "    sum_bouts = [flies[fly].fictrac.fictrac['Yh'][bout[align_to]-before:bout[align_to]+after] for bout in all_bouts[bout_type]][1:-1]\n",
    "    sum_bouts = np.asarray(sum_bouts)\n",
    "    #avg_bout = np.mean(sum_bouts,axis=0)\n",
    "    \n",
    "    return xss, yss, sum_bouts\n",
    "    \n",
    "def plot_bout_triggered(xss, yss, behavior_trace, x_pos, y_pos):\n",
    "    before = 3000 #in ms\n",
    "    after = 3000 # in ms\n",
    "    before = int(before/10) # now everything is in units of 10ms\n",
    "    after = int(after/10)\n",
    "    ax = fig.add_axes([x_pos,y_pos,.05,.05])\n",
    "    plt.plot(xss,yss,marker=',',linestyle='',color='k') # for plotting individual neural points\n",
    "\n",
    "    neural_bin_size = 50\n",
    "    neural_bins = np.arange(-before*10,after*10,neural_bin_size)\n",
    "    bin_id = np.digitize(xss.ravel(), neural_bins)\n",
    "    avgs = []\n",
    "    for i in range(len(neural_bins)):\n",
    "        avgs.append(np.mean(yss.ravel()[np.where(bin_id==i)[0]]))\n",
    "    plt.plot(neural_bins-0.5*neural_bin_size,avgs,linewidth=0.5,color='red')\n",
    "\n",
    "    plt.axvline(0,linestyle='--',color='k',linewidth=0.5)\n",
    "    #plt.xlabel('ms')\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    plt.axhline(0,linestyle='--',color='k',linewidth=0.5)\n",
    "    plt.ylim(-1.5,2)\n",
    "    #plt.xlim(-1000,1000)\n",
    "    #plt.title(align_to,y=.85,backgroundcolor='white')\n",
    "\n",
    "    # Plot behavior\n",
    "\n",
    "    bins = bbb.create_bins(10,before*10,after*10)[:-1]\n",
    "    plt.plot(bins,behavior_trace/3,color='blue',linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will have a vector of timepoints of onsets\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = 20\n",
    "fly_names = ['fly_087', 'fly_089', 'fly_094', 'fly_097', 'fly_098', 'fly_099', 'fly_100', 'fly_101', 'fly_105']\n",
    "dataset_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "expt_len = 1000*30*60\n",
    "resolution = 10\n",
    "high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fly:\n",
    "    def __init__ (self, fly_name, fly_idx):\n",
    "        self.dir = os.path.join(dataset_path, fly_name, 'func_0')\n",
    "        self.fly_idx = fly_idx\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "    def load_timestamps (self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "    def load_fictrac (self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "    def load_brain_slice (self):\n",
    "        self.brain = brain[:,:,:,self.fly_idx]\n",
    "    def load_anatomy (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', 'anat-to-meanbrain.nii')\n",
    "        self.anatomy = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def load_z_depth_correction (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', '20201220_warped_z_depth.nii')\n",
    "        self.z_correction = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def get_cluster_averages (self, cluster_model_labels, n_clusters):\n",
    "        neural_data = self.brain.reshape(-1, 3384)\n",
    "        signals = []\n",
    "        self.cluster_indicies = []\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model_labels==cluster_num)[0]\n",
    "            mean_signal = np.mean(neural_data[cluster_indicies,:], axis=0)\n",
    "            signals.append(mean_signal)\n",
    "            self.cluster_indicies.append(cluster_indicies) # store for later\n",
    "        self.cluster_signals=np.asarray(signals)\n",
    "    def get_cluster_id (self, x, y):\n",
    "        ax_vec = x*128 + y\n",
    "        for i in range(n_clusters):\n",
    "            if ax_vec in self.cluster_indicies[i]:\n",
    "                cluster_id = i\n",
    "                break\n",
    "        return cluster_id\n",
    "\n",
    "class Fictrac:\n",
    "    def __init__ (self, fly_dir, timestamps):\n",
    "        self.fictrac_raw = bbb.load_fictrac(os.path.join(fly_dir, 'fictrac'))\n",
    "        self.timestamps = timestamps\n",
    "    def make_interp_object(self, behavior):\n",
    "        # Create camera timepoints\n",
    "        fps=50\n",
    "        camera_rate = 1/fps * 1000 # camera frame rate in ms\n",
    "        expt_len = 1000*30*60\n",
    "        x_original = np.arange(0,expt_len,camera_rate)\n",
    "\n",
    "        # Smooth raw fictrac data\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(np.asarray(self.fictrac_raw[behavior]),25,3)\n",
    "\n",
    "        # Create interp object with camera timepoints\n",
    "        fictrac_interp_object = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "        return fictrac_interp_object\n",
    "\n",
    "    def pull_from_interp_object(self, interp_object, timepoints):\n",
    "        new_interp = interp_object(timepoints)\n",
    "        np.nan_to_num(new_interp, copy=False);\n",
    "        return new_interp\n",
    "\n",
    "    def interp_fictrac(self, z):\n",
    "        behaviors = ['dRotLabY', 'dRotLabZ']; shorts = ['Y', 'Z']\n",
    "        self.fictrac = {}\n",
    "\n",
    "        for behavior, short in zip(behaviors, shorts):\n",
    "            interp_object = self.make_interp_object(behavior)\n",
    "            self.fictrac[short + 'i'] = interp_object\n",
    "\n",
    "            ### Velocity ###\n",
    "            low_res_behavior = self.pull_from_interp_object(interp_object, self.timestamps[:,z])\n",
    "            self.fictrac[short] = low_res_behavior#/np.std(low_res_behavior)\n",
    "\n",
    "            ### Clipped Velocities ###\n",
    "            self.fictrac[short + '_pos'] = np.clip(self.fictrac[short], a_min=0, a_max=None)\n",
    "            self.fictrac[short + '_neg'] = np.clip(self.fictrac[short], a_min=None, a_max=0)*-1\n",
    "\n",
    "            ### Strongly Clipped Velocities ###\n",
    "            # excludes points even close to 0\n",
    "            #self.fictrac[short + '_pos_very'] = np.clip(self.fictrac[short], a_min=0.3, a_max=None)\n",
    "            #self.fictrac[short + '_neg_very'] = np.clip(self.fictrac[short], a_min=None, a_max=-0.3)*-1\n",
    "\n",
    "            ### Acceleration ###\n",
    "            high_res_behavior = self.pull_from_interp_object(interp_object, high_res_timepoints)\n",
    "            self.fictrac[short + 'h'] = high_res_behavior/np.std(high_res_behavior)\n",
    "\n",
    "            accel = scipy.signal.savgol_filter(np.diff(high_res_behavior),25,3)\n",
    "            accel = np.append(accel, 0)\n",
    "            interp_object = interp1d(high_res_timepoints, accel, bounds_error = False)\n",
    "            acl = interp_object(self.timestamps[:,z])\n",
    "            acl[-1] = 0\n",
    "            self.fictrac[short + 'a'] = acl#/np.std(acl)\n",
    "\n",
    "            ### Clipped Acceleration ###\n",
    "            self.fictrac[short + 'a' + '_pos'] = np.clip(self.fictrac[short + 'a'], a_min=0, a_max=None)\n",
    "            self.fictrac[short + 'a' + '_neg'] = np.clip(self.fictrac[short + 'a'], a_min=None, a_max=0)*-1\n",
    "\n",
    "        self.fictrac['YZ'] = np.sqrt(np.power(self.fictrac['Y'],2), np.power(self.fictrac['Z'],2))\n",
    "        self.fictrac['YZh'] = np.sqrt(np.power(self.fictrac['Yh'],2), np.power(self.fictrac['Zh'],2))\n",
    "\n",
    "def find_bouts(fly):\n",
    "    expt_len = 1000*30*60\n",
    "    resolution = 10\n",
    "    high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "\n",
    "    behavior = 'Yh'\n",
    "\n",
    "    B_THRESHOLD = np.std(flies[fly].fictrac.fictrac[behavior])/4\n",
    "    ALIVE_TIME = 1000 # in ms\n",
    "    DEAD_TIME = 1000 # in ms\n",
    "\n",
    "    state = 'quiescent'\n",
    "    up_streak = 0\n",
    "    down_streak = 0\n",
    "    BOUTS = []\n",
    "    ALIVE_TIME = int(ALIVE_TIME/resolution)\n",
    "    DEAD_TIME = int(DEAD_TIME/resolution)\n",
    "\n",
    "    for i in range(len(flies[fly].fictrac.fictrac[behavior])):\n",
    "        # If high behavior, reset down_streak, and add 1 to up_streak\n",
    "        if flies[fly].fictrac.fictrac[behavior][i] > B_THRESHOLD:\n",
    "            down_streak = 0\n",
    "            up_streak += 1\n",
    "        else:\n",
    "            up_streak = 0\n",
    "            down_streak += 1\n",
    "\n",
    "        if state == 'quiescent':\n",
    "            if up_streak >= ALIVE_TIME:\n",
    "                state = 'moving'\n",
    "                BOUTS.append({'start': i-ALIVE_TIME})\n",
    "        elif state == 'moving':\n",
    "            if down_streak >= DEAD_TIME:\n",
    "                state = 'quiescent'\n",
    "                BOUTS[-1]['end'] = i-DEAD_TIME\n",
    "    BOUTS = [bout for bout in BOUTS if 'end' in bout]\n",
    "    #print('Found {} bouts'.format(len(BOUTS)))\n",
    "\n",
    "    ### Clean Start Bouts\n",
    "    # remove bouts that have behavior too close *before* them\n",
    "    before = 1000 # in ms\n",
    "    before = int(before/10)\n",
    "    start_bouts=[bout for bout in BOUTS if np.mean(np.abs(flies[fly].fictrac.fictrac[behavior][bout['start']-before:bout['start']])) < .2]\n",
    "    #print('Remaining start_bouts post-cleaning: {}'.format(np.shape(start_bouts)[0]))\n",
    "\n",
    "    ### Clean Stop Bouts\n",
    "    # remove bouts that have behavior too close *after* them\n",
    "    before = 1000 # in ms\n",
    "    before = int(before/10)\n",
    "    stop_bouts=[bout for bout in BOUTS if np.mean(np.abs(flies[fly].fictrac.fictrac[behavior][bout['end']:bout['end']+before])) < .2]\n",
    "    #print('Remaining stop_bouts bouts post-cleaning: {}'.format(np.shape(stop_bouts)[0]))\n",
    "    return start_bouts, stop_bouts\n",
    "\n",
    "def bout_triggered(fly, neural_data, all_bouts, bout_type, original_z):\n",
    "    if bout_type == 'start_bouts':\n",
    "        align_to = 'start'\n",
    "    elif bout_type == 'stop_bouts':\n",
    "        align_to = 'end'\n",
    "    before = 3000 #in ms\n",
    "    after = 3000 # in ms\n",
    "    jump = flies[fly].timestamps[1,0]-flies[fly].timestamps[0,0]\n",
    "    num_neural_points = int(before/jump)\n",
    "\n",
    "    before = int(before/10) # now everything is in units of 10ms\n",
    "    after = int(after/10)\n",
    "    bins = bbb.create_bins(10,before*10,after*10)[:-1]\n",
    "\n",
    "    xss = []; yss = []\n",
    "    for i in range(len(all_bouts[bout_type])):\n",
    "        nearest = np.searchsorted(flies[fly].timestamps[:,original_z]/10, all_bouts[bout_type][i][align_to])\n",
    "        offset = (flies[fly].timestamps[nearest,original_z]/10 - all_bouts[bout_type][i][align_to])*10\n",
    "        xs = np.arange(offset-num_neural_points*jump,offset+num_neural_points*jump,jump)\n",
    "        ys = neural_data[nearest-num_neural_points:nearest+num_neural_points]\n",
    "        if len(ys) == 10:\n",
    "            xss.append(xs); yss.append(ys)\n",
    "    xss = np.asarray(xss); yss = np.asarray(yss)\n",
    "\n",
    "    sum_bouts = [flies[fly].fictrac.fictrac['Yh'][bout[align_to]-before:bout[align_to]+after] for bout in all_bouts[bout_type]][1:-1]\n",
    "    sum_bouts = np.asarray(sum_bouts)\n",
    "    #avg_bout = np.mean(sum_bouts,axis=0)\n",
    "\n",
    "    return xss, yss, sum_bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### Load Superslice ###\n",
    "#######################\n",
    "brain_file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/superslice_{}.nii\".format(z) #<---------- !!!\n",
    "brain = np.array(nib.load(brain_file).get_data(), copy=True)\n",
    "fly_idx_delete = 3 #(fly_095)\n",
    "brain = np.delete(brain, fly_idx_delete, axis=-1) #### DELETING FLY_095 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### Load Clusters ###\n",
    "#####################\n",
    "n_clusters = 2000\n",
    "labels_file = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/final_9_cluster_labels_2000.npy'\n",
    "cluster_model_labels = np.load(labels_file) #z,t\n",
    "cluster_model_labels = cluster_model_labels[z,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 85.07 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 4.06 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 92.60 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 4.15 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 60.02 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.82 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 64.02 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.46 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 66.66 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 4.01 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 87.33 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.61 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 75.18 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.48 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 78.30 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.56 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 71.44 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.44 sec\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "### Build Flies ###\n",
    "###################\n",
    "flies = {}\n",
    "for i, fly in enumerate(fly_names):\n",
    "    flies[fly] = Fly(fly_name=fly, fly_idx=i)\n",
    "    flies[fly].load_timestamps()\n",
    "    flies[fly].load_fictrac()\n",
    "    flies[fly].fictrac.interp_fictrac(z)\n",
    "    flies[fly].load_brain_slice()\n",
    "    flies[fly].load_z_depth_correction()\n",
    "    flies[fly].get_cluster_averages(cluster_model_labels, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "### Main Loop ###\n",
    "#################\n",
    "xss_master = []; yss_master = []; sum_bouts_master = []\n",
    "for cluster_num in range(n_clusters):\n",
    "    print(str(cluster_num))\n",
    "\n",
    "    # loop over flies\n",
    "    bout_type = 'start_bouts'\n",
    "    pooled_bouts = {'xss': np.empty((0,10)), 'yss': np.empty((0,10)), 'sum_bouts': np.empty((0,600))}\n",
    "    for fly in fly_names:\n",
    "        # Get bout times\n",
    "        start_bouts, stop_bouts = find_bouts(fly)\n",
    "        all_bouts = {'start_bouts': start_bouts, 'stop_bouts': stop_bouts}\n",
    "        neural_data = flies[fly].cluster_signals[cluster_num, :]\n",
    "\n",
    "        # Get original_Z\n",
    "        cluster_indicies = flies[fly].cluster_indicies[cluster_num]\n",
    "        z_map = flies[fly].z_correction[:,:,z].ravel()\n",
    "        original_z = int(np.median(z_map[cluster_indicies]))\n",
    "\n",
    "        # Get bout triggered neural vectors\n",
    "        xss, yss, sum_bouts = bout_triggered(fly, neural_data, all_bouts, bout_type, original_z)\n",
    "\n",
    "        # Collect output from all flies\n",
    "        pooled_bouts['xss'] = np.vstack((pooled_bouts['xss'], xss))\n",
    "        pooled_bouts['yss'] = np.vstack((pooled_bouts['yss'], yss))\n",
    "        pooled_bouts['sum_bouts'] = np.vstack((pooled_bouts['sum_bouts'], sum_bouts))\n",
    "    # Collect output from all supervoxels\n",
    "    xss_master.append(pooled_bouts['xss'])\n",
    "    yss_master.append(pooled_bouts['yss'])\n",
    "    sum_bouts_master.append(pooled_bouts['sum_bouts'])\n",
    "    \n",
    "xss_master = np.asarray(xss_master)\n",
    "yss_master = np.asarray(yss_master)\n",
    "sum_bouts_master = np.asarray(sum_bouts_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
