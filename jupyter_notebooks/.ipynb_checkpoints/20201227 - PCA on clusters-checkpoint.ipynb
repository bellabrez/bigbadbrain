{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages/lib/python/')\n",
    "import ants\n",
    "import os\n",
    "import bigbadbrain as bbb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from skimage.filters import threshold_triangle\n",
    "sys.path.insert(0, '/home/users/brezovec/.local/lib/python3.6/site-packages')\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import cv2\n",
    "import matplotlib.patches as mpatches\n",
    "import psutil\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import itertools\n",
    "import random\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from scipy.fftpack import fft,fftshift,ifft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/superslice_{}.nii\".format(z) #<---------- !!!\n",
    "brain = np.array(nib.load(brain_file).get_data(), copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 128, 3384, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=20\n",
    "n_clusters = 2000\n",
    "labels_file = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/cluster_labels.npy'\n",
    "cluster_model_labels = np.load(labels_file)\n",
    "cluster_model_labels = cluster_model_labels[z,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_names = ['fly_087', 'fly_089', 'fly_094', 'fly_095', 'fly_097', 'fly_098', 'fly_099', 'fly_100', 'fly_101', 'fly_105']\n",
    "dataset_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "expt_len = 1000*30*60\n",
    "resolution = 10\n",
    "high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n",
    "\n",
    "class Fly:\n",
    "    def __init__ (self, fly_name, fly_idx):\n",
    "        self.dir = os.path.join(dataset_path, fly_name, 'func_0')\n",
    "        self.fly_idx = fly_idx\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "    def load_timestamps (self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "    def load_fictrac (self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "    def load_brain_slice (self):\n",
    "        self.brain = brain[:,:,:,self.fly_idx]\n",
    "    def load_anatomy (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', 'anat-to-meanbrain.nii')\n",
    "        self.anatomy = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def load_z_depth_correction (self):\n",
    "        to_load = os.path.join(dataset_path, self.fly_name, 'warp', '20201220_warped_z_depth.nii')\n",
    "        self.z_correction = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "    def get_cluster_averages (self, cluster_model_labels, n_clusters):\n",
    "        neural_data = self.brain.reshape(-1, 3384)\n",
    "        signals = []\n",
    "        self.cluster_indicies = []\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model_labels==cluster_num)[0]\n",
    "            mean_signal = np.mean(neural_data[cluster_indicies,:], axis=0)\n",
    "            signals.append(mean_signal)\n",
    "            self.cluster_indicies.append(cluster_indicies) # store for later\n",
    "        self.cluster_signals=np.asarray(signals)\n",
    "    def get_cluster_id (self, x, y):\n",
    "        ax_vec = x*128 + y\n",
    "        for i in range(n_clusters):\n",
    "            if ax_vec in self.cluster_indicies[i]:\n",
    "                cluster_id = i\n",
    "                break\n",
    "        return cluster_id\n",
    "\n",
    "class Fictrac:\n",
    "    def __init__ (self, fly_dir, timestamps):\n",
    "        self.fictrac_raw = bbb.load_fictrac(os.path.join(fly_dir, 'fictrac'))\n",
    "        self.timestamps = timestamps\n",
    "    def make_interp_object(self, behavior):\n",
    "        # Create camera timepoints\n",
    "        fps=50\n",
    "        camera_rate = 1/fps * 1000 # camera frame rate in ms\n",
    "        expt_len = 1000*30*60\n",
    "        x_original = np.arange(0,expt_len,camera_rate)\n",
    "\n",
    "        # Smooth raw fictrac data\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(np.asarray(self.fictrac_raw[behavior]),25,3)\n",
    "\n",
    "        # Create interp object with camera timepoints\n",
    "        fictrac_interp_object = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "        return fictrac_interp_object\n",
    "\n",
    "    def pull_from_interp_object(self, interp_object, timepoints):\n",
    "        new_interp = interp_object(timepoints)\n",
    "        np.nan_to_num(new_interp, copy=False);\n",
    "        return new_interp\n",
    "\n",
    "    def interp_fictrac(self, z):\n",
    "        behaviors = ['dRotLabY', 'dRotLabZ']; shorts = ['Y', 'Z']\n",
    "        self.fictrac = {}\n",
    "\n",
    "        for behavior, short in zip(behaviors, shorts):\n",
    "            interp_object = self.make_interp_object(behavior)\n",
    "            self.fictrac[short + 'i'] = interp_object\n",
    "\n",
    "            ### Velocity ###\n",
    "            low_res_behavior = self.pull_from_interp_object(interp_object, self.timestamps[:,z])\n",
    "            self.fictrac[short] = low_res_behavior#/np.std(low_res_behavior)\n",
    "\n",
    "            ### Clipped Velocities ###\n",
    "            self.fictrac[short + '_pos'] = np.clip(self.fictrac[short], a_min=0, a_max=None)\n",
    "            self.fictrac[short + '_neg'] = np.clip(self.fictrac[short], a_min=None, a_max=0)*-1\n",
    "\n",
    "            ### Strongly Clipped Velocities ###\n",
    "            # excludes points even close to 0\n",
    "            #self.fictrac[short + '_pos_very'] = np.clip(self.fictrac[short], a_min=0.3, a_max=None)\n",
    "            #self.fictrac[short + '_neg_very'] = np.clip(self.fictrac[short], a_min=None, a_max=-0.3)*-1\n",
    "\n",
    "            ### Acceleration ###\n",
    "            high_res_behavior = self.pull_from_interp_object(interp_object, high_res_timepoints)\n",
    "            self.fictrac[short + 'h'] = high_res_behavior/np.std(high_res_behavior)\n",
    "\n",
    "            accel = scipy.signal.savgol_filter(np.diff(high_res_behavior),25,3)\n",
    "            accel = np.append(accel, 0)\n",
    "            interp_object = interp1d(high_res_timepoints, accel, bounds_error = False)\n",
    "            acl = interp_object(self.timestamps[:,z])\n",
    "            acl[-1] = 0\n",
    "            self.fictrac[short + 'a'] = acl#/np.std(acl)\n",
    "\n",
    "            ### Clipped Acceleration ###\n",
    "            self.fictrac[short + 'a' + '_pos'] = np.clip(self.fictrac[short + 'a'], a_min=0, a_max=None)\n",
    "            self.fictrac[short + 'a' + '_neg'] = np.clip(self.fictrac[short + 'a'], a_min=None, a_max=0)*-1\n",
    "\n",
    "        self.fictrac['YZ'] = np.sqrt(np.power(self.fictrac['Y'],2), np.power(self.fictrac['Z'],2))\n",
    "        self.fictrac['YZh'] = np.sqrt(np.power(self.fictrac['Yh'],2), np.power(self.fictrac['Zh'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 73.31 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 2.99 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 44.82 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 2.99 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 45.53 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.02 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 78.88 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.20 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 55.20 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.06 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 58.62 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.19 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 55.87 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.17 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 53.75 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.12 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 58.22 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.19 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 59.22 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 3.15 sec\n"
     ]
    }
   ],
   "source": [
    "flies = {}\n",
    "for i, fly in enumerate(fly_names):\n",
    "    flies[fly] = Fly(fly_name=fly, fly_idx=i)\n",
    "    flies[fly].load_timestamps()\n",
    "    flies[fly].load_fictrac()\n",
    "    flies[fly].fictrac.interp_fictrac(z)\n",
    "    flies[fly].load_brain_slice()\n",
    "    flies[fly].load_z_depth_correction()\n",
    "    flies[fly].get_cluster_averages(cluster_model_labels, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "cluster_signals = []\n",
    "for cluster_num in range(n_clusters):\n",
    "    all_fly_neural = []\n",
    "    if cluster_num%100 == 0:\n",
    "        print(cluster_num)\n",
    "    for fly in fly_names:\n",
    "        signal = flies[fly].cluster_signals[cluster_num,:]\n",
    "        all_fly_neural.extend(signal)\n",
    "    Y = np.asarray(all_fly_neural)\n",
    "    cluster_signals.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_signals = np.asarray(cluster_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 33840)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = PCA().fit(cluster_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
